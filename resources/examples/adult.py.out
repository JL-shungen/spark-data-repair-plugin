Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) 

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Python version 3.6.8 (default, Dec 29 2018 19:04:46)
SparkSession available as 'spark'.
Scavenger APIs (version 0.1.0-spark3.0-EXPERIMENTAL) available as 'scavenger'.
>>> # Loads a target data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/adult.csv") \
...   .write \
...   .saveAsTable("adult")
>>> 
>>> scavenger.repair().misc() \
...   .setDbName("default") \
...   .setTableName("adult") \
...   .setRowId("tid") \
...   .flatten() \
...   .write \
...   .saveAsTable("adult_flatten")
>>> 
>>> spark.table("adult").show(1)
+---+-----+------------+------------+----+-------------+-----------+
|tid|  Age|   Education|  Occupation| Sex|      Country|     Income|
+---+-----+------------+------------+----+-------------+-----------+
|  0|31-50|Some-college|Craft-repair|Male|United-States|LessThan50K|
+---+-----+------------+------------+----+-------------+-----------+
only showing top 1 row

>>> spark.table("adult_flatten").show(1)
+---+---------+-----+
|tid|attribute|value|
+---+---------+-----+
|  0|      Age|31-50|
+---+---------+-----+
only showing top 1 row

>>> # Loads a ground truth data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/adult_clean.csv") \
...   .write \
...   .saveAsTable("adult_clean")
>>> 
>>> spark.table("adult_flatten") \
...   .join(spark.table("adult_clean"), ["tid", "attribute"], "inner") \
...   .where("not(value <=> correct_val)") \
...   .write \
...   .saveAsTable("error_cells_ground_truth")
>>> 
>>> spark.table("adult_clean").show(1)
+---+---------+-----------+
|tid|attribute|correct_val|
+---+---------+-----------+
|  0|      Age|      31-50|
+---+---------+-----------+
only showing top 1 row

>>> spark.table("error_cells_ground_truth").show(1)
+---+---------+-----+-----------+
|tid|attribute|value|correct_val|
+---+---------+-----+-----------+
|  3|      Sex| null|       Male|
+---+---------+-----+-----------+
only showing top 1 row

>>> # Detects error cells then repairs them
... repaired_df = scavenger.repair() \
...   .setDbName("default") \
...   .setTableName("adult") \
...   .setRowId("tid") \
...   .setErrorDetector(ConstraintErrorDetector(constraint_path="./testdata/adult_constraints.txt")) \
...   .run()
Detecting errors in a table `default.adult` (20 rows x 7 cols)...
...
>>> # Computes performance numbers (precision & recall)
... #  - Precision: the fraction of correct repairs, i.e., repairs that match
... #    the ground truth, over the total number of repairs performed
... #  - Recall: correct repairs over the total number of errors
... pdf = repaired_df.join(spark.table("adult_clean"), ["tid", "attribute"], "inner")
>>> rdf = repaired_df.join(spark.table("error_cells_ground_truth"), ["tid", "attribute"], "right_outer")
>>> 
>>> precision = pdf.where("repaired <=> correct_val").count() / pdf.count()
>>> recall = rdf.where("repaired <=> correct_val").count() / rdf.count()
>>> f1 = (2.0 * precision * recall) / (precision + recall)                      
>>> 
>>> print("Precision={} Recall={} F1={}".format(precision, recall, f1))
Precision=1.0 Recall=1.0 F1=1.0

