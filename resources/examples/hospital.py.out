Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) 

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Python version 3.6.8 (default, Dec 29 2018 19:04:46)
SparkSession available as 'spark'.
Delphi APIs (version 0.1.0-spark3.0-EXPERIMENTAL) available as 'delphi'.
>>> # Loads a target data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/hospital.csv") \
...   .write \
...   .saveAsTable("hospital")
>>> 
>>> delphi.repair().misc \
...   .options({"db_name": "default", "table_name": "hospital", "row_id": "tid"}) \
...   .flatten() \
...   .write \
...   .saveAsTable("hospital_flatten")
>>> 
>>> spark.table("hospital").show(1)
+---+--------------+--------------------+--------------------+--------+--------+----------+-----+-------+----------+-----------+--------------------+--------------------+----------------+--------------------+-----------+--------------------+-----+------+--------------+
|tid|ProviderNumber|        HospitalName|            Address1|Address2|Address3|      City|State|ZipCode|CountyName|PhoneNumber|        HospitalType|       HospitalOwner|EmergencyService|           Condition|MeasureCode|         MeasureName|Score|Sample|      Stateavg|
+---+--------------+--------------------+--------------------+--------+--------+----------+-----+-------+----------+-----------+--------------------+--------------------+----------------+--------------------+-----------+--------------------+-----+------+--------------+
|  0|         10018|callahan eye foun...|1720 university blvd|    null|    null|birmingham|   al|  35233| jefferson| 2053258100|acute care hospitals|voluntary non-pro...|             yes|surgical infectio...|scip-card-2|surgery patients ...| null|  null|al_scip-card-2|
+---+--------------+--------------------+--------------------+--------+--------+----------+-----+-------+----------+-----------+--------------------+--------------------+----------------+--------------------+-----------+--------------------+-----+------+--------------+
only showing top 1 row

>>> spark.table("hospital_flatten").show(1)
+---+--------------+-----+
|tid|     attribute|value|
+---+--------------+-----+
|  0|ProviderNumber|10018|
+---+--------------+-----+
only showing top 1 row

>>> 
>>> # Loads a ground truth data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/hospital_clean.csv") \
...   .write \
...   .saveAsTable("hospital_clean")
>>> 
>>> spark.table("hospital_flatten") \
...   .join(spark.table("hospital_clean"), ["tid", "attribute"], "inner") \
...   .where("not(value <=> correct_val)") \
...   .write \
...   .saveAsTable("error_cells_ground_truth")
>>> 
>>> spark.table("hospital_clean").show(1)
+---+--------------+-----------+
|tid|     attribute|correct_val|
+---+--------------+-----------+
|  0|ProviderNumber|      10018|
+---+--------------+-----------+
only showing top 1 row

>>> spark.table("error_cells_ground_truth").show(1)
+---+-----------+--------------------+--------------------+
|tid|  attribute|               value|         correct_val|
+---+-----------+--------------------+--------------------+
|  0|MeasureName|surgery patients ...|surgery patients ...|
+---+-----------+--------------------+--------------------+
only showing top 1 row

>>> 
>>> # Detects error cells then repairs them
... from repair.errors import NullErrorDetector, ConstraintErrorDetector
... error_detectors = [
...     ConstraintErrorDetector(constraint_path="./testdata/hospital_constraints.txt"),
...     NullErrorDetector()
... ]
... val repaired_df = delphi.repair \
...   .setDbName("default") \
...   .setTableName("hospital") \
...   .setRowId("tid") \
...   .setErrorDetectors(error_detectors) \
...   .setRuleBasedModelEnabled(True) \
...   .option("model.hp.no_progress_loss", "100") \
...   .setDiscreteThreshold(100) \
...   .run()
Detecting errors in a table `default.hospital` (1000 rows x 20 cols)...
...

>>> 
>>> # Computes performance numbers (precision & recall)
... #  - Precision: the fraction of correct repairs, i.e., repairs that match
... #    the ground truth, over the total number of repairs performed
... #  - Recall: correct repairs over the total number of errors
... pdf = repaired_df \
...   .join(spark.table("hospital_clean"), ["tid", "attribute"], "inner")
>>> rdf = repaired_df \
...   .join(spark.table("error_cells_ground_truth"), ["tid", "attribute"], "right_outer")
>>> 
>>> precision = pdf.where("repaired <=> correct_val").count() / pdf.count()
>>> recall = rdf.where("repaired <=> correct_val").count() / rdf.count()        
>>> f1 = (2.0 * precision * recall) / (precision + recall)
>>> 
>>> print("Precision={} Recall={} F1={}".format(precision, recall, f1))
Precision=0.9473684210526315 Recall=0.8888888888888888 F1=0.9171974522292994

