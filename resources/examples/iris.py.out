Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) 

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Python version 3.6.8 (default, Dec 29 2018 19:04:46)
SparkSession available as 'spark'.
Delphi APIs (version 0.1.0-spark3.0-EXPERIMENTAL) available as 'delphi'.
>>> # Loads a target data then defines tables for it
... iris_schema = "tid string, sepal_length double, sepal_width double, petal_length double, petal_width double"
>>> spark.read \
...   .option("header", True) \
...   .schema(iris_schema) \
...   .csv("./testdata/iris.csv") \
...   .write \
...   .saveAsTable("iris")
>>> 
>>> delphi.repair().misc \
...   .options({"db_name": "default", "table_name": "iris", "row_id": "tid"}) \
...   .flatten() \
...   .write \
...   .saveAsTable("iris_flatten")
>>> 
>>> spark.table("iris").show(1)
+---+------------+-----------+------------+-----------+
|tid|sepal_length|sepal_width|petal_length|petal_width|
+---+------------+-----------+------------+-----------+
|  0|         5.1|        3.5|         1.4|        0.2|
+---+------------+-----------+------------+-----------+
only showing top 1 row

>>> spark.table("iris_flatten").show(1)
+---+------------+-----+
|tid|   attribute|value|
+---+------------+-----+
|  0|sepal_length|  5.1|
+---+------------+-----+
only showing top 1 row

>>> 
>>> # Loads a ground truth data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/iris_clean.csv") \
...   .write \
...   .saveAsTable("iris_clean")
>>> 
>>> spark.table("iris_flatten") \
...   .join(spark.table("iris_clean"), ["tid", "attribute"], "inner") \
...   .where("not(value <=> correct_val)") \
...   .write \
...   .saveAsTable("error_cells_ground_truth")
>>> 
>>> spark.table("iris_clean").show(1)
+---+------------+-----------+
|tid|   attribute|correct_val|
+---+------------+-----------+
|  0|sepal_length|        5.1|
+---+------------+-----------+
only showing top 1 row

>>> spark.table("error_cells_ground_truth").show(1)
+---+-----------+-----+-----------+
|tid|  attribute|value|correct_val|
+---+-----------+-----+-----------+
|  7|sepal_width| null|        3.4|
+---+-----------+-----+-----------+
only showing top 1 row

>>> 
>>> # Detects error cells then repairs them
... val repaired_df = delphi.repair \
...   .setDbName("default") \
...   .setTableName("iris") \
...   .setRowId("tid") \
...   .run()
>>> 
>>> # Compares predicted values with the correct ones
>>> cmp_df = repaired_df.join(spark.table("iris_clean"), ["tid", "attribute"], "inner")
>>> cmp_df.orderBy("attribute").show()
+---+------------+-------------+------------------+-----------+
|tid|   attribute|current_value|          repaired|correct_val|
+---+------------+-------------+------------------+-----------+
| 55|sepal_length|         null|5.9923797193715655|        5.7|
| 81|sepal_length|         null| 5.427886780984579|        5.5|
| 82|sepal_length|         null|  5.44507438452237|        5.8|
|111|sepal_length|         null| 6.044092911873808|        6.4|
|144|sepal_length|         null| 6.560128418603836|        6.7|
| 75|sepal_length|         null| 6.135800714371424|        6.6|
| 95|sepal_length|         null|5.7422157494513675|        5.7|
| 56|sepal_length|         null| 6.352834536217649|        6.3|
|131|sepal_length|         null| 7.094875890371557|        7.9|
| 18|sepal_length|         null| 5.239805334127158|        5.7|
| 43|sepal_length|         null|5.2688710765683355|        5.0|
|128|sepal_length|         null| 6.527317303573899|        6.4|
|130|sepal_length|         null| 7.207596233879159|        7.4|
| 89| sepal_width|         null|2.5500331221744346|        2.5|
|135| sepal_width|         null| 3.053870135011956|        3.0|
| 92| sepal_width|         null| 2.582685193522234|        2.6|
| 19| sepal_width|         null|3.6159576175010226|        3.8|
|125| sepal_width|         null|2.9671957396124973|        3.2|
| 20| sepal_width|         null|  3.75707655736584|        3.4|
| 22| sepal_width|         null|3.1530450549738767|        3.6|
+---+------------+-------------+------------------+-----------+
only showing top 20 rows

>>> # Show a scatter plog for repaired/correct_val values
... import matplotlib.pyplot as plt
... g = cmp_df.selectExpr("double(repaired)", "double(correct_val)").toPandas().plot.scatter(x="correct_val", y="repaired")
... plt.show(g)

>>> # Computes performance numbers for continous attributes (RMSE/MAE)
... n = repaired_df.count()
>>> rmse = cmp_df.selectExpr(f"sqrt(sum(pow(correct_val - repaired, 2.0)) / {n}) rmse") \
...     .collect()[0] \
...     .rmse
>>> mae = cmp_df.selectExpr(f"sum(abs(correct_val - repaired)) / {n} mae") \
...     .collect()[0] \
...     .mae
>>> print(f"RMSE={rmse} MAE={mae} RMSE/MAE={rmse/mae}")
RMSE=0.34279849631928416 MAE=0.2760441800207878 RMSE/MAE=1.2418247553470223

