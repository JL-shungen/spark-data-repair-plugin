Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) 

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Python version 3.6.8 (default, Dec 29 2018 19:04:46)
SparkSession available as 'spark'.
Delphi APIs (version 0.1.0-spark3.0-EXPERIMENTAL) available as 'delphi'.
>>> # Loads a target data then defines tables for it
... boston_schema = "tid int, CRIM double, ZN int, INDUS string, CHAS string, NOX string, RM double, AGE string, DIS double, RAD string, TAX int, PTRATIO string, B double, LSTAT double"
>>> spark.read \
...   .option("header", True) \
...   .schema(boston_schema) \
...   .csv("./testdata/boston.csv") \
...   .write \
...   .saveAsTable("boston")
>>> 
>>> delphi.repair().misc \
...   .options({"db_name": "default", "table_name": "boston", "row_id": "tid"}) \
...   .flatten() \
...   .write \
...   .saveAsTable("boston_flatten")
>>> 
>>> spark.table("boston").show(1)
+---+-------+----+-----+----+-----+-----+----+----+---+-----+-------+-----+-----+
|tid|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE| DIS|RAD|  TAX|PTRATIO|    B|LSTAT|
+---+-------+----+-----+----+-----+-----+----+----+---+-----+-------+-----+-----+
|  0|0.00632|18.0| 2.31|null|0.538|6.575|65.2|4.09|1.0|296.0|   15.3|396.9| 4.98|
+---+-------+----+-----+----+-----+-----+----+----+---+-----+-------+-----+-----+
only showing top 1 row

>>> spark.table("boston_flatten").show(1)
+---+---------+-------+
|tid|attribute|  value|
+---+---------+-------+
|  0|     CRIM|0.00632|
+---+---------+-------+
only showing top 1 row

>>> 
>>> # Loads a ground truth data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/boston_clean.csv") \
...   .write \
...   .saveAsTable("boston_clean")
>>> 
>>> spark.table("boston_flatten") \
...   .join(spark.table("boston_clean"), ["tid", "attribute"], "inner") \
...   .where("not(value <=> correct_val)") \
...   .write \
...   .saveAsTable("error_cells_ground_truth")
>>> 
>>> spark.table("boston_clean").show(1)
+---+---------+-----------+
|tid|attribute|correct_val|
+---+---------+-----------+
|  0|     CRIM|    0.00632|
+---+---------+-----------+
only showing top 1 row

>>> spark.table("error_cells_ground_truth").show(1)
+---+---------+-----+-----------+
|tid|attribute|value|correct_val|
+---+---------+-----+-----------+
|  0|     CHAS| null|        0.0|
+---+---------+-----+-----------+
only showing top 1 row

>>> 
>>> # Detects error cells then repairs them
... val repaired_df = delphi.repair \
...   .setDbName("default") \
...   .setTableName("boston") \
...   .setRowId("tid") \
...   .setDiscreteThreshold(30) \
...   .option("hp.no_progress_loss", "300") \
...   .run()
Detecting errors in a table `default.boston` (506 rows x 14 cols)...
...
>>> 
>>> # Computes performance numbers for discrete attributes (precision & recall)
... #  - Precision: the fraction of correct repairs, i.e., repairs that match
... #    the ground truth, over the total number of repairs performed
... #  - Recall: correct repairs over the total number of errors
... is_discrete = "attribute NOT IN ('CRIM', 'LSTAT')"
>>> pdf = repaired_df \
...   .where(is_discrete) \
...   .join(spark.table("boston_clean"), ["tid", "attribute"], "inner")
>>>
>>> ground_truth_df = spark.table("error_cells_ground_truth") \
...   .where(is_discrete)
>>> rdf = repaired_df \
...   .where(is_discrete) \
...   .join(ground_truth_df, ["tid", "attribute"], "right_outer")
>>> 
>>> precision = pdf.where("repaired <=> correct_val").count() / pdf.count()
>>> recall = rdf.where("repaired <=> correct_val").count() / rdf.count()
>>> f1 = (2.0 * precision * recall) / (precision + recall)
>>> 
>>> print("Precision={} Recall={} F1={}".format(precision, recall, f1))
Precision=0.6470588235294118 Recall=0.6470588235294118 F1=0.6470588235294118
>>> 
>>> # Computes performance numbers for continous attributes (RMSE)
... is_continous = f"NOT({is_discrete)"
... continous_pdf = pdf.where(is_continous)
...
>>> # Show a scatter plog for repaired/correct_val values
... import matplotlib.pyplot as plt
... g = continous_pdf.selectExpr("double(repaired)", "double(correct_val)").toPandas().plot.scatter(x="correct_val", y="repaired")
... plt.show(g)
...
>>> n = continous_pdf.count()
... rmse = continous_pdf.selectExpr(f"sqrt(sum(pow(correct_val - repaired, 2.0)) / {n}) rmse") \
...     .collect()[0] \
...     .rmse
... mae = continous_pdf.selectExpr(f"sum(abs(correct_val - repaired)) / {n} mae") \
...     .collect()[0] \
...     .mae
...
>>> print(f"RMSE={rmse} MAE={mae} RMSE/MAE={rmse/mae}")
RMSE=2.4058233737433725 MAE=1.8800883431373354 RMSE/MAE=1.2796331526255484

