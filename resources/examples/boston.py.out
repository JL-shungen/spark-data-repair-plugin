Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) 

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Python version 3.6.8 (default, Dec 29 2018 19:04:46)
SparkSession available as 'spark'.
Scavenger APIs (version 0.1.0-spark3.0-EXPERIMENTAL) available as 'scavenger'.
>>> # Loads a target data then defines tables for it
... boston_schema = "tid string, CRIM double, ZN string, INDUS string, CHAS string, NOX string, RM double, AGE string, DIS double, RAD string, TAX string, PTRATIO string, B double, LSTAT double"
>>> spark.read \
...   .option("header", True) \
...   .schema(boston_schema) \
...   .csv("./testdata/boston.csv") \
...   .write \
...   .saveAsTable("boston")
>>> 
>>> scavenger.repair().misc() \
...   .setDbName("default") \
...   .setTableName("boston") \
...   .setRowId("tid") \
...   .flatten() \
...   .write \
...   .saveAsTable("boston_flatten")
>>> 
>>> spark.table("boston").show(1)
+---+-------+----+-----+----+-----+-----+----+----+---+-----+-------+-----+-----+
|tid|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE| DIS|RAD|  TAX|PTRATIO|    B|LSTAT|
+---+-------+----+-----+----+-----+-----+----+----+---+-----+-------+-----+-----+
|  0|0.00632|18.0| 2.31|null|0.538|6.575|65.2|4.09|1.0|296.0|   15.3|396.9| 4.98|
+---+-------+----+-----+----+-----+-----+----+----+---+-----+-------+-----+-----+
only showing top 1 row

>>> spark.table("boston_flatten").show(1)
+---+---------+-------+
|tid|attribute|  value|
+---+---------+-------+
|  0|     CRIM|0.00632|
+---+---------+-------+
only showing top 1 row

>>> 
>>> # Loads a ground truth data then defines tables for it
... spark.read \
...   .option("header", True) \
...   .csv("./testdata/boston_clean.csv") \
...   .write \
...   .saveAsTable("boston_clean")
>>> 
>>> spark.table("boston_flatten") \
...   .join(spark.table("boston_clean"), ["tid", "attribute"], "inner") \
...   .where("not(value <=> correct_val)") \
...   .write \
...   .saveAsTable("error_cells_ground_truth")
>>> 
>>> spark.table("boston_clean").show(1)
+---+---------+-----------+
|tid|attribute|correct_val|
+---+---------+-----------+
|  0|     CRIM|    0.00632|
+---+---------+-----------+
only showing top 1 row

>>> spark.table("error_cells_ground_truth").show(1)
+---+---------+-----+-----------+
|tid|attribute|value|correct_val|
+---+---------+-----+-----------+
|  0|     CHAS| null|        0.0|
+---+---------+-----+-----------+
only showing top 1 row

>>> 
>>> # Detects error cells then repairs them
... val repaired_df = scavenger.repair() \
...   .setDbName("default") \
...   .setTableName("boston") \
...   .setRowId("tid") \
...   .run()
Detecting errors in a table `default.boston` (506 rows x 14 cols)...
...
>>> 
>>> # Computes performance numbers for discrete attributes (precision & recall)
... #  - Precision: the fraction of correct repairs, i.e., repairs that match
... #    the ground truth, over the total number of repairs performed
... #  - Recall: correct repairs over the total number of errors
... is_discrete = "attribute NOT IN ('CRIM', 'LSTAT')"
>>> pdf = repaired_df \
...   .where(is_discrete) \
...   .join(spark.table("boston_clean"), ["tid", "attribute"], "inner")
>>>
>>> ground_truth_df = spark.table("error_cells_ground_truth") \
...   .where(is_discrete)
>>> rdf = repaired_df \
...   .where(is_discrete) \
...   .join(ground_truth_df, ["tid", "attribute"], "right_outer")
>>> 
>>> precision = pdf.where("repaired <=> correct_val").count() / pdf.count()
>>> recall = rdf.where("repaired <=> correct_val").count() / rdf.count()
>>> f1 = (2.0 * precision * recall) / (precision + recall)
>>> 
>>> print("Precision={} Recall={} F1={}".format(precision, recall, f1))
Precision=0.8733333333333333 Recall=0.5177865612648221 F1=0.6501240694789083
>>> 
>>> # Computes performance numbers for continous attributes (RMSE)
... is_continous = f"NOT({is_discrete)"
>>> n = repaired_df.count()
>>> rmse = repaired_df \
...   .where(is_continous) \
...   .join(spark.table("boston_clean"), ["tid", "attribute"], "inner") \
...   .selectExpr(f"sqrt(sum(pow(correct_val - repaired, 2.0)) / {n}) rmse") \
...   .collect()[0] \
...   .rmse
>>> 
>>> print(f"RMSE={rmse}")
RMSE=2.0626999993860706

