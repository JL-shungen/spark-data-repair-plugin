#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# Shell script for submitting a job to repair table data

set -e -o pipefail

if [ -z "${SPARK_HOME}" ]; then
  echo "env SPARK_HOME not defined" 1>&2
  exit 1
elif ! command -v conda &> /dev/null; then
  echo "You should install conda first" 1>&2
  exit 1
fi

# Sets the root directory
ROOT_DIR="$(cd "`dirname $0`"/..; pwd)"

# Loads some variables from `pom.xml`
. ${ROOT_DIR}/bin/package.sh && get_package_variables_from_pom "${ROOT_DIR}"

find_package() {
  local _BUILT_PACKAGE="${ROOT_DIR}/target/${PACKAGE_JAR_NAME}"
  if [ -e "$_BUILT_PACKAGE" ]; then
    PACKAGE=$_BUILT_PACKAGE
  else
    PACKAGE="${ROOT_DIR}/assembly/${PACKAGE_JAR_NAME}"
    echo "${_BUILT_PACKAGE} not found, so use pre-compiled ${PACKAGE}" 1>&2
  fi
}

# Do some preparations before submiting a job
find_package

# Check if a virtual env already exists or not
PYSPARK_CONDA_ENV="pyspark-py37-conda-env"
if [ -z "$(conda env list | grep "$PYSPARK_CONDA_ENV ")" ]; then
  conda env create -q -n $PYSPARK_CONDA_ENV python=3.7 --file ${ROOT_DIR}/bin/conda-reduced.yml
fi

# Activate a conda virtual env for a driver
source activate $PYSPARK_CONDA_ENV

# Package the required dependencies for executors if necessary
PYSPARK_EXECUTOR_ENV=${ROOT_DIR}/bin/pyspark-py37-conda-env.tar.gz
if [ ! -f "$PYSPARK_EXECUTOR_ENV" ]; then
  conda pack -q -f -o $PYSPARK_EXECUTOR_ENV
fi

# Then, submit a job with given arguments
# PYSPARK_DRIVER_PYTHON=python
# export PYSPARK_PYTHON=./environment/bin/python
${SPARK_HOME}/bin/spark-submit \
  --jars=${PACKAGE} \
  --archives ${PYSPARK_EXECUTOR_ENV}#environment \
  --py-files ${ROOT_DIR}/python/lib/repair.zip \
  "$@"

