#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# Shell script for submitting a job to repair table data

set -e -o pipefail

if [ -z "${SPARK_HOME}" ]; then
  echo "env SPARK_HOME not defined" 1>&2
  exit 1
fi

# Sets the root directory
ROOT_DIR="$(cd "`dirname $0`"/..; pwd)"

# Loads some variables from `pom.xml`
. ${ROOT_DIR}/bin/package.sh && get_package_variables_from_pom "${ROOT_DIR}"

find_package() {
  local _BUILT_PACKAGE="${ROOT_DIR}/target/${PACKAGE_JAR_NAME}"
  if [ -e "$_BUILT_PACKAGE" ]; then
    PACKAGE=$_BUILT_PACKAGE
  else
    PACKAGE="${ROOT_DIR}/assembly/${PACKAGE_JAR_NAME}"
    echo "${_BUILT_PACKAGE} not found, so use pre-compiled ${PACKAGE}" 1>&2
  fi
}

# Do some preparations before launching pyspark
find_package

# Activate a conda virtual env for required dependencies
# TODO: Needs to put all the dependencies in `python/lib/repair.zip`
. ${ROOT_DIR}/bin/conda.sh && activate_conda_virtual_env "${ROOT_DIR}"

# Then, submit a job with given arguments
${SPARK_HOME}/bin/spark-submit --jars=${PACKAGE} --py-files ${ROOT_DIR}/python/lib/repair.zip "$@"

